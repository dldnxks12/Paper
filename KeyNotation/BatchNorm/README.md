## Batch Norm?

그동안 batch norm에 대한 얘기를 찾아보면 항상 수식이 먼저 나와서 잘 읽지 않았던 기억이 난다.

수식적인 부분은 일단 배제하고, 그 의미를 잘 정리해보자

<br>

#### 기존 방법의 문제점

신경망이 안정적으로 잘 학습되기 위해서는 입력 layer에 넣을 입력과 각 층의 Weight들을 표준화할 필요가 있다.

      입력과 Weight의 분포가 표준화되어 있지 않다면 학습이 잘 되지 않는다.

따라서 Batch Norm 방법이 고안되기 전에는 Manual하게 입력을 Centering과 Scaling하고, 입력 뉴런 n개의 층의 Weight를 /sqrt(n/2)로 표준화했다.

간단하지만 이렇게만 해도 위와 같은 방법을 쓰지 않았을 때에 비해 더 빨리 좋은 성능으로 수렴하는 것을 확인할 수 있다.

하지만 여기서 문제가 발생한다.

      '입력층 Only'에 대해서만 정규화가 가능했다는 것!
      
      내부의 은닉층에 대해서는 표준화할 방법이 없었다.

<br>

#### 은닉층을 표준화하지 못한다면?

은닉층을 표준화하지 못한 상태에서는 은닉층의 입력 데이터들의 분포가 학습이 진행됨에 따라 계~속 변한다.

      입력 데이터의 분포의 형태가 유지되지 않으므로 당연히 학습도 잘 진행되지 않는다. 

      Gradient 값의 변화가 큰 초기 상태일수록 그 영향이 크다!
    
<br>

#### Internel Covariate Shift - 내부 공분산 변화

위에서 언급한 문제를 '내부 공분산 변화' 문제라고 한다.

은닉층의 입력도 표준화한다면 안정적으로 깊~은 layer의 가중치도 학습시킬 수 있을 것이다!

      '은닉층의 입력을 표준화한다'는 것은 곧 '이전 층의 출력을 표준화한다'와 같은 의미이다.
      

<br>

#### 그래도 문제점이 있다?

이렇게 은닉층의 입력을 표준화하면 Gradient Update 과정에서 Bias의 값이 무시된다. 

[link 참고](https://de-novo.org/2018/05/28/batch-normalization-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0/)

<br>

#### Bias가 무시되는 문제를 해결! Batch Norm 

위의 문제를 해결하기 위해 표준화한 후 scaling 과 shifting한 raw activation을 사용한다. 자세한 내용은 위 link 참고 

따라서 여전히 bias는 무시되지만, beta value가 bias를 대체하며 업데이트된다. 
      

